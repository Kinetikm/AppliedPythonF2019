{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Mining vs Text Mining\n",
    "\n",
    "Data Mining:   \n",
    "* извлечение *неочевидной* информации\n",
    "\n",
    "Text Mining:  \n",
    "* извлечение *очевидной* информации\n",
    "\n",
    "Трудности  \n",
    "* Огромные объемы\n",
    "* Отстутсвие структуры\n",
    "\t    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Задачи Text Mining\n",
    "* Суммаризация текста  \n",
    "агрегация новостей\n",
    "* Классификация и кластеризация документов  \n",
    "категоризация, антиспам, sentiment analysis, opinion mining\n",
    "* Извлечение метаданных  \n",
    "определение языка, автора, тегирование\n",
    "* Выделение сущностей  \n",
    "места, люди, компании, почтовые адреса\n",
    "\n",
    "\n",
    "* автоматический перевод\n",
    "* чат-бот\n",
    "* поиск точных ответов на вопросы в тексте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Этапы (простой) обработки текста\n",
    "\n",
    "<img src=\"images/textm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Декодирование\n",
    "\n",
    "\n",
    "**Def.**  \n",
    "перевод последовательности байт в последовательность символов\n",
    "\n",
    "* Распаковка  \n",
    "*plain/.zip/.gz/...*\n",
    "* Кодировка  \n",
    "*ASCII/utf-8/Windows-1251/...*\n",
    "* Формат  \n",
    "*csv/xml/json/doc...*\n",
    "\n",
    "Кроме того: что такое документ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Разбиение на токены\n",
    "**Def.**  \n",
    "разбиение последовательности символов на части (токены), возможно, исключая из рассмотрения некоторые символы  \n",
    "Наивный подход: разделить строку пробелами и выкинуть знаки препинания  \n",
    "\n",
    "\n",
    "*Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.*  \n",
    "\n",
    "\n",
    "**Проблемы:**  \n",
    "* example@example.com, 127.0.0.1\n",
    "* С++, C#\n",
    "* York University vs New York University\n",
    "* Зависимость от языка (“Lebensversicherungsgesellschaftsangestellter”, “l’amour”)\n",
    "Альтернатива: n-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "s = \"Трисия любила Нью-Йорк, поскольку любовь к Нью-Йорку могла положительно повлиять на ее карьеру.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"\\w+|[^\\w\\s]+\")\n",
    "for t in tokenizer.tokenize(s): \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Стоп-слова\n",
    "**Def.**  \n",
    "Наиболее частые слова в языке, не содержащие никакой информации о содержании текста\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "print(\" \".join(stopwords.words(\"russian\")[1:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Проблема: “To be or not to be\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Нормализация\n",
    "**Def.**  \n",
    "Приведение токенов к единому виду для того, чтобы избавиться от поверхностной разницы в написании  \n",
    "\n",
    "Подходы  \n",
    "* сформулировать набор правил, по которым преобразуется токен  \n",
    "Нью-Йорк → нью-йорк → ньюйорк → ньюиорк\n",
    "* явно хранить связи между токенами (WordNet – Princeton)  \n",
    "машина → автомобиль, Windows 6→ window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "s = \"Нью-Йорк\"\n",
    "s1 = s.lower()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "s2 = re.sub(r\"\\W\", \"\", s1, flags=re.U)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "s3 = re.sub(r\"й\", u\"и\", s2, flags=re.U)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Стемминг и Лемматизация\n",
    "**Def.**  \n",
    "Приведение грамматических форм слова и однокоренных слов к единой основе (lemma):\n",
    "* Stemming – с помощью простых эвристических правил\n",
    "  * Porter (Cambridge – 1980)\n",
    "        5 этапов, на каждом применяется набор правил, таких как\n",
    "            sses → ss (caresses → caress)\n",
    "            ies → i (ponies → poni)\n",
    "\n",
    "  * Lovins (1968)\n",
    "  * Paice (1990)\n",
    "  * другие\n",
    "* Lemmatization – с использованием словарей и морфологического анализа\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "\n",
    "\n",
    "s = PorterStemmer()\n",
    "print(s.stem(\"Tokenization\"))\n",
    "print(s.stem(\"stemming\"))\n",
    "\n",
    "r = RussianStemmer()\n",
    "print(r.stem(\"Авиация\"))\n",
    "print(r.stem(\"национальный\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Наблюдение**  \n",
    "для сложных языков лучше подходит лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "print(morph.parse(\"думающему\")[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heaps' law\n",
    "Эмпирическая закономерность в лингвистике, описывающая распределение числа уникальных слов в документе (или наборе документов) как функцию от его длины.\n",
    "\n",
    "$$\n",
    "M = k T^\\beta, \\;M \\text{ -- размер словаря}, \\; T \\text{ -- количество слов в корпусе}\n",
    "$$\n",
    "$$\n",
    "30 \\leq k \\leq 100, \\; b \\approx 0.5\n",
    "$$\n",
    "\n",
    "<img src=\"images/dim.png\">\n",
    "<img src=\"images/heaps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Представление документов\n",
    "**Boolean Model.** Присутствие или отсутствие слова в документе  \n",
    "**Bag of Words.** Порядок токенов не важен  \n",
    "\n",
    "*Погода была ужасная, принцесса была прекрасная.\n",
    "Или все было наоборот?*\n",
    "\n",
    "Координаты\n",
    "* Мультиномиальные: количество токенов в документе\n",
    "* Числовые: взвешенное количество токенов в документе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Zipf's law\n",
    "Эмпирическая закономерность распределения частоты слов естественного языка\n",
    "\n",
    "$t_1, \\ldots, t_N$ - токены, отранжированные по убыванию частоты\n",
    "   \t\n",
    "$f_1, \\dots, f_N$ - соответствующие частоты\n",
    "\n",
    "**Закон Ципфа**\n",
    "\t$$\n",
    "\tf_i = \\frac{c}{i^k}\n",
    "\t$$\t\n",
    "\t\n",
    "\tЧто еще? Посещаемость сайтов, количество друзей, население городов...\n",
    "<img src=\"images/zipf.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "Количество вхождений слова $t$ в документе $d$\n",
    "$$\n",
    "TF_{t,d} = term\\!\\!-\\!\\!frequency(t, d)\n",
    "$$\n",
    "Количество документов из $N$ возможных, где встречается $t$\n",
    "$$\n",
    "DF_t = document\\!\\!-\\!\\!fequency(t)\n",
    "$$\n",
    "$$\n",
    "IDF_t = inverse\\!\\!-\\!\\!document\\!\\!-\\!\\!frequency(t) = \\log \\frac{N}{DF_t}\n",
    "$$\n",
    "TF-IDF\n",
    "$$\n",
    "TF\\!\\!-\\!\\!IDF_{t,d} = TF_{t,d} \\times IDF_t\n",
    "$$\n",
    "\n",
    "Оценивает важность слова в контексте документа, являющегося частью корпуса\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25\n",
    "Функция ранжирования, используемая поисковыми системами для упорядочивания документов по их релевантности данному поисковому запросу. BM25 и его различные более поздние модификации (например, BM25F) представляют собой современные TF-IDF-подобные функции ранжирования, широко используемые на практике в поисковых системах  \n",
    "\n",
    "$$\n",
    "score(D, Q) = \\sum_{i=1}^{n} IDF(q_i) * \\frac{TF(q_i, D) * (k_1 + 1)}{TF(q_i, D) + k_1 * (1 - b + b * \\frac{|D|}{avgdl})}\n",
    "$$\n",
    "где:  \n",
    "* $Q$ -- запрос, содержащий слова $q_1, q_2, ..., q_n$;\n",
    "* $D$ -- документ, $|D|$ -- длина документа;\n",
    "* $avgdl$ -- средняя длина документа в коллекции;\n",
    "* $k_1$ и $b$ -- свободные коэффициенты, обычно их выбирают как $k_1=2.0$ и $b=0.75$;\n",
    "\n",
    "$IDF$ чаще всего сглаженный:\n",
    "$$\n",
    "IDF(q_i) = \\log{\\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}}\n",
    "$$\n",
    "\n",
    "Заметим, что вышеуказанная формула $IDF$ имеет следующий недостаток. Для слов, входящих в более чем половину документов из коллекции, значение IDF отрицательно. Таким образом, при наличии любых двух почти идентичных документов, в одном из которых есть слово, а в другом — нет, второй может получить бо́льшую оценку.  \n",
    "Иными словами, часто встречающиеся слова испортят окончательную оценку документа. Это нежелательно, поэтому во многих приложениях вышеприведённая формула может быть скорректирована следующими способами:\n",
    "* Игнорировать вообще все отрицательные слагаемые в сумме (что эквивалентно занесению в стоп-лист и игнорированию всех соответствующих высокочастотных слов);\n",
    "* Налагать на IDF некоторую нижнюю границу $\\varepsilon$ : $IDF(q_i) = \\min(IDF(q_i), \\varepsilon)$;\n",
    "* Использовать другую формулу $IDF$, не принимающую отрицательных значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практический кейс\n",
    "### Автоматическое определение текста языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import codecs\n",
    "import unicodedata\n",
    "import operator\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import cross_val_score, train_test_split\n",
    "except:\n",
    "    from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем решать задачу определения языка печатного текста. В файле `europarl.test.txt` содержатся записи депатов в Европарламенте. Каждая строка содержит код языка и высказывание на этом языке, например, на болгарском:\n",
    "\n",
    "`bg\t(DE) Г-н председател, след повече от 300 години колониално управление и след като континентът се превърна в арена на Студената война, днес Латинска Америка вече е един от нововъзникващите региони в света.`\n",
    "\n",
    "Код языка будет целевой переменной, а из высказывания нам предстоит извлечь признаки.\n",
    "\n",
    "Один из возможных подходов состоит в том, чтобы в качестве признаков использовать тройки из подряд идущих символов, встречающихся в словах. Предположение состоит в том, что для каждого языка список наиболее популярных троек более-менее уникален. Попробуем проверить это предположение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считывание данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_PATH = \"./europarl.test.txt\" # Path to the data file\n",
    "N_GRAM = 3 # Extract symbol sequences of length N\n",
    "TOP_TOKENS = 10 # Number of top selected n-grams for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(data_path):\n",
    "    \"\"\"\n",
    "    Reads a sequence of documents from the text file\n",
    "    located on a given path.\n",
    "\n",
    "    Returns:\n",
    "        A generator of tuples (LANG_CODE, unicode)\n",
    "    \"\"\"\n",
    "    with codecs.open(data_path, \"rU\") as data_file:\n",
    "        for line in data_file:\n",
    "            lang, doc = line.strip().split(\"\\t\")\n",
    "            yield lang, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_document(doc):\n",
    "    \"\"\"\n",
    "    Convert document to lower-case and remove accents\n",
    "    \n",
    "    Returns:\n",
    "        A normalised document as unicode\n",
    "    \"\"\"\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", doc.lower()) if not unicodedata.combining(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_document(doc, n):\n",
    "    \"\"\"\n",
    "    Split document in N-Grams\n",
    "\n",
    "    Returns:\n",
    "        Iterable (generator or list) of unicode n-grams\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.WordPunctTokenizer()\n",
    "    for token in tokenizer.tokenize(doc):\n",
    "        if len(token) >= n:\n",
    "            for ngram in nltk.ngrams(token, n):\n",
    "                yield \"\".join(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым делом нам необходимо зачитать данные из файла. Будем читать 3 структуры данных:\n",
    "\n",
    "- docs - список словарей, каждый из которых соответствует одному документу и содержит количество вхождений для каждой n-граммы (токена)\n",
    "- langs - список, содержащий классы документов (каждому коду языка соответствует числовой класс)\n",
    "- lang_freq - словарь, который нужен для подсчета ниболее популярных n-грам для каждого языка. Элементы этого словаря: код языка -> (id класса, частоты n-грам (аналогично docs)) \n",
    "\n",
    "Для того, чтобы заработал код, зачитывающий данные, необходимо (до) реализовать функции, перечисленные выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of dicts, each representing one document in format:\n",
    "# {token: count1, ...}\n",
    "docs = []\n",
    "# Language code for each dict (0-based)\n",
    "langs = []\n",
    "# A list of tuples, each tuple corresponds to one language\n",
    "# First compunent is the code of the language, second is its token frequencies\n",
    "# Contains entries like {lang_code: (lang_id, {token_frequencies})}\n",
    "lang_freq = {}\n",
    "\n",
    "for lang, doc in read_documents(DS_PATH):\n",
    "    normalized_doc = normalise_document(doc)\n",
    "\n",
    "    token_freq = {}\n",
    "    for token in tokenize_document(normalized_doc, N_GRAM):\n",
    "        token_freq[token] = 1 + token_freq.get(token, 0)\n",
    "        if lang not in lang_freq:\n",
    "            lang_freq[lang] = (len(lang_freq), {})\n",
    "        lang_freq[lang][1][token] = 1 + lang_freq[lang][1].get(token, 0)\n",
    "\n",
    "    docs.append(token_freq)\n",
    "    langs.append(lang_freq[lang][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков\n",
    "\n",
    "Здесь предстоит выбрать топовые n-граммы для каждого языка (`select_features`) и отфильтровать документы так, чтобы в них остались только отобранные (`keep_only_features`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(lang_freq, top_tokens):\n",
    "    \"\"\"\n",
    "    From each language selects top_tokens to be used as features\n",
    "    \n",
    "    TODO: Implement this\n",
    "\n",
    "    Returns:\n",
    "        set(unicode tokens)\n",
    "    \"\"\"\n",
    "    features = set()\n",
    "    for lang, (lid, token_freq) in lang_freq.items():\n",
    "        features.add(\"xam\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_features(docs, features):\n",
    "    \"\"\"\n",
    "    Removes non-feature tokens from the document representations\n",
    "    \"\"\"\n",
    "    for token_freq in docs:\n",
    "        for token in list(token_freq.keys()):\n",
    "            if token not in features:\n",
    "                del token_freq[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top n features for each lang\n",
    "features = select_features(lang_freq, TOP_TOKENS)\n",
    "# Remove from documents all features except the selected\n",
    "keep_only_features(docs, features)\n",
    "\n",
    "# Transform documents to numpy matrix\n",
    "dv = DictVectorizer()\n",
    "x = dv.fit_transform(docs).todense()\n",
    "y = numpy.array(langs)\n",
    "print(\"Data set shape x=({} x {}) y={}\".format(x.shape[0], x.shape[1], len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание и настройка модели\n",
    "\n",
    "В этом пункте требуется реализовать модель (NB) и оценить метрику `accuracy` на кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    Initialise an NB model, supported by Sklearn\n",
    "\n",
    "    Returns:\n",
    "        Sklearn model instance\n",
    "    \"\"\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, x, y, folds=10):\n",
    "    \"\"\"\n",
    "    Computes cross-validation score for the given data set and model.\n",
    "    \n",
    "    TODO: Implement this\n",
    "\n",
    "    Returns:\n",
    "        A numpy.array of accuracy scores.\n",
    "    \"\"\"\n",
    "    return [0.9, 0.9, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(model, x, y, class_ind=0):    \n",
    "    # Compute ROC curve\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.5, random_state=0)\n",
    "    fit = model.fit(x_train, y_train)\n",
    "    y_prob = fit.predict_proba(x_test)   \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob[:, class_ind], pos_label=class_ind)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Plot ROC curve\n",
    "    plt.fill_between(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc, alpha=0.3)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic for class index %s' % class_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_Замечание_ : обратите внимание, что тут нужно реализовать перебор параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement parameter grid search here\n",
    "model = create_model()\n",
    "# Print cross-validated accuracy\n",
    "scores = validate_model(model, x, y)\n",
    "print(\"Model mean accuracy: {}\".format(numpy.mean(scores)))\n",
    "\n",
    "# Plot ROC\n",
    "plt.figure(figsize=(8, 8))\n",
    "plot_roc(model, x, y, 0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
