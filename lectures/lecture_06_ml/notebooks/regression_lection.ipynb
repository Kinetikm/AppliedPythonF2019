{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "jupyter nbconvert regression_lection.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1> Regression </h1>\n",
    "\n",
    "#### структура\n",
    "- Введение - зачем нужна регрессия\n",
    "- Постановка задачи\n",
    "- Линейная регрессия пример решения\n",
    "- Вспомним про нормальное распределение. Его свойства. ЦПТ. Работы Гальтона и Фишера.\n",
    "- Линейная регрессия - вывод через правдоподобие\n",
    "- Линейная регрессия с нелинейными базисными функциями\n",
    "- Переобучение. Регуляризация.\n",
    "- bias variance trade_off\n",
    "- Вероятностный взгляд на регуляризацию\n",
    "\n",
    "\n",
    "Что почитать\n",
    "\n",
    "https://habr.com/company/ods/blog/323890/\n",
    "\n",
    "https://habr.com/company/ods/blog/322076/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:28.069227Z",
     "start_time": "2018-10-24T07:47:27.588923Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Для чего нужна регрессия?\n",
    "- Чтобы узнать зависимость \n",
    "\n",
    "![](./imgs/1.jpg)\n",
    "\n",
    "\n",
    "- что будет с акциями завтра?\n",
    "- как быстро к вам доберется курьер или такси подаст машину?\n",
    "- сколько будет стоить квартира с определенной площадью и удаленностью до метро?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Чуток формальнее\n",
    "- $X$ - матрица-описание объектов, дизайн-матрица;\n",
    "- $Y$ - матрица-характеристики объектов;\n",
    "- $f$ - зависимость - некоторый метод подсчета характеристик объектов основываясь на их описании;\n",
    "- $\\left(X, Y = f\\left(X\\right)\\right)$ - наблюдения;\n",
    "- $S = \\left\\{X_i, Y_i\\right\\}_{i=1}^N$ - обучающая выборка - набор $N$ наблюдений - пары (описание объекта - значение);\n",
    "- $\\hat f$ - регрессионная модель - функция которая аппроксимирует исходную зависимость $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае линейная регрессия моделируется так:\n",
    "\n",
    "$$\n",
    "y_i = w_0 + w_1 * x_{i1} + ... + w_m * x_{im} + \\epsilon \n",
    "$$\n",
    "\n",
    "Где $\\epsilon$ - шум, обычно предполагают $\\epsilon \\sim \\mathcal{N}(0,\\,\\sigma^{2})$ - с нулевым средним.\n",
    "\n",
    "Задача сводится к ее обучению (поиску коэффициентов) для некоторой метрики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### 1. Инженеристое решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:31.447655Z",
     "start_time": "2018-10-24T07:47:31.436862Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1132)\n",
    "\n",
    "X = np.random.rand(60) * 30\n",
    "y = X * 3 + 6 + np.random.normal(scale=10, size=X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:32.404229Z",
     "start_time": "2018-10-24T07:47:32.104199Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:32.611535Z",
     "start_time": "2018-10-24T07:47:32.406055Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'r.')\n",
    "y_hat = X * 2 + 4\n",
    "plt.plot(X, y_hat, 'b-')\n",
    "plt.title('разность = {}'.format(np.sqrt(np.sum(y_hat - y)**2)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:32.922998Z",
     "start_time": "2018-10-24T07:47:32.672269Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'r.')\n",
    "y_hat = X * 4.5 + 5.5\n",
    "plt.plot(X, y_hat, 'b-')\n",
    "plt.title('разность = {}'.format(np.sqrt(np.sum(y_hat - y)**2)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:33.412476Z",
     "start_time": "2018-10-24T07:47:33.176878Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'r.')\n",
    "y_hat = X * 3 + 6\n",
    "plt.plot(X, y_hat, 'b-')\n",
    "plt.title('разность = {}'.format(np.sqrt(np.sum(y_hat - y)**2)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Как выбрать оптимальзую прямую при условии, что в жизни точки не лежат на одной прямой из-за (например) погрешности изменений или неучтенных факторов ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T06:38:17.445519Z",
     "start_time": "2018-10-24T06:38:17.407476Z"
    }
   },
   "source": [
    "$$\n",
    "ERROR(\\hat{f}, x) = \\frac{1}{N} \\sum_{i=1}^{N}\\left(\\ y_i - \\hat{f}(x_i)\\ \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "ERROR(\\hat{f}, x) = \\frac{1}{N} \\sum_{i=1}^{N}\\left(\\ y_i - \\hat{f}(x_i)\\ \\right) ^ 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "ERROR(\\hat{f}, x) = \\frac{1}{N} \\sum_{i=1}^{N}\\left|\\ y_i - \\hat{f}(x_i)\\ \\right|\n",
    "$$\n",
    "\n",
    "$$\n",
    "ERROR(\\hat{f}, x) = \\frac{1}{N} \\sum_{i=1}^{N}\\left|\\ \\log(y_i) - \\log(\\hat{f}(x_i))\\ \\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы обучать регрессионные модели нужно определиться с тем как измерять качество предсказаний.\n",
    "\n",
    "Обозначим за $\\hat{f}$ - прогноз модели, за y - истинные ответы, за $L(y,\\hat{f})$ - отклонение прогноза a от истинного ответа y.\n",
    "\n",
    "Основной способ измерить отклонение - посчитать квадрат отклонений\n",
    "\n",
    "$$\n",
    "L(y,\\hat{f}) = (y - \\hat{f}) ^ 2\n",
    "$$\n",
    "\n",
    "Квадрат отклонения - дифференцируемая выпуклая функция. Наиболее часто встречается в задачах регрессии.\n",
    "\n",
    "- Основанный на ней функционал называется MSE (Mean Squared Error)\n",
    "\n",
    "$$\n",
    "MSE(\\hat{f}, x) = \\frac{1}{N} \\sum_{i=1}^{N}\\left(\\ y_i - \\hat{f}(x_i)\\ \\right) ^ 2\n",
    "$$\n",
    "\n",
    "\n",
    "- MAE (Mean Absolute Error)\n",
    "\n",
    "$$\n",
    "MAE(\\hat{f}, x) = \\frac{1}{N} \\sum_{i=1}^{N}\\left|\\ y_i - \\hat{f}(x_i)\\ \\right|\n",
    "$$\n",
    "\n",
    "\n",
    "- $R^2$ - коэффициент детерминации, показывает долю объясненной дисперсии (т.е. доля дисперсии объясненная моделью) в общей дисперсии целевой переменной.\n",
    "\n",
    "Данный коэффициент принимает значение от 0 до 1 (чем ближе к 1, тем лучше модель объясняет данные)\n",
    "\n",
    "(https://ru.wikipedia.org/wiki/Коэффициент_детерминации)\n",
    "\n",
    "$$\n",
    "R^2(\\hat{f}, x) = 1 - \\frac{\\sum_{i=1}^{N}\\left(\\ y_i - \\hat{f}(x_i)\\ \\right) ^ 2}{\\sum_{i=1}^{N}\\left(\\ y_i - \\bar{y}\\ \\right) ^ 2} \n",
    "$$, где\n",
    "\n",
    "$\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:33.971645Z",
     "start_time": "2018-10-24T07:47:33.717009Z"
    }
   },
   "outputs": [],
   "source": [
    "# - как найти минимум фукнции f(x) ?\n",
    "\n",
    "def f(x):\n",
    "    return (x + 2) ** 2 + 3\n",
    "\n",
    "plt.plot(range(-8, 4), [f(i) for i in range(-8, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:52:45.499914Z",
     "start_time": "2018-10-24T12:52:45.239687Z"
    }
   },
   "outputs": [],
   "source": [
    "def df_dx(x):\n",
    "    return 2 * (x + 2)\n",
    "\n",
    "plt.plot(range(-8, 4), [ df_dx(i) for i in range(-8, 4) ], label='df/dx')\n",
    "plt.plot(range(-8, 4), [ 0 for i in range(-8, 4) ], label='y = 0')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:34.202400Z",
     "start_time": "2018-10-24T07:47:34.174785Z"
    }
   },
   "outputs": [],
   "source": [
    "# подобие градиентного спуска.\n",
    "\n",
    "x_ = 10\n",
    "for i in range(100):\n",
    "    x_ += - df_dx(x_) * 0.1\n",
    "    print(x_, f(x_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![asd](http://www.machinelearning.ru/wiki/images/thumb/f/f6/Grad1.PNG/500px-Grad1.PNG)\n",
    "\n",
    "http://www.machinelearning.ru/wiki/index.php?title=Метод_градиентного_спуска"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "функция потерь - минимизируем сумму квадратов отклонений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{y} = k * x + b\n",
    "\\\\\n",
    "Q(x) = \\sum_{i=0}^{N}{(y_i - \\hat{y_i})^2}\n",
    "\\\\\n",
    "Q(x) = \\sum_{i=0}^{N}(y_i - k * x_i - b) ^ 2\n",
    "\\\\\n",
    "\\frac{d Q(x)}{d k} = 2 * \\sum_{i=0}^{N}(y_i - k * x_i - b) * (- x_i) = 0 \n",
    "\\\\\n",
    "\\frac{d Q(x)}{d b} = 2 * \\sum_{i=0}^{N}(y_i - k * x_i - b) * (-1) = 0\n",
    "\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k: \n",
    "$$\n",
    "\\frac{d Q(x)}{d k} = - 2 * \\sum_{i=0}^{N}(y_i * x_i - k * x_i ^ 2 - b * x_i) = 0\n",
    "\\\\\n",
    "\\frac{d Q(x)}{d k} = \\sum_{i=0}^{N}(y_i * x_i) - \\sum_{i=0}^{N} (k * x_i ^ 2) - \\sum_{i=0}^{N}( b * x_i) = 0\n",
    "\\\\\n",
    "k * \\sum_{i=0}^{N} (x_i ^ 2) + b * \\sum_{i=0}^{N}( x_i) =  \\sum_{i=0}^{N}(y_i * x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b:\n",
    "$$\n",
    "\\frac{d Q(x)}{d b} = 2 * \\sum_{i=0}^{N}(y_i - k * x_i - b) * (-1) = 0\n",
    "\\\\\n",
    "\\sum_{i=0}^{N}(y_i - b) =  \\sum_{i=0}^{N}(k * x_i)\n",
    "\\\\\n",
    "\\sum_{i=0}^{N}y_i = N * b + k * \\sum_{i=0}^{N}(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "k * \\sum_{i=0}^{N} (x_i ^ 2) + b * \\sum_{i=0}^{N}( x_i) =  \\sum_{i=0}^{N}(y_i * x_i)\n",
    "\\\\\n",
    "\\\\\n",
    "\\sum_{i=0}^{N}y_i = N * b + k * \\sum_{i=0}^{N}(x_i)\n",
    "\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Обозначим\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{N}\\sum_{i=0}^{N} x_i\n",
    "\\\\\n",
    "\\bar{y} = \\frac{1}{N}\\sum_{i=0}^{N} y_i\n",
    "\\\\\n",
    "\\overline{xy} = \\frac{1}{N}\\sum_{i=0}^{N} y_i*x_i\n",
    "\\\\\n",
    "\\overline{x^2} = \\frac{1}{N}\\sum_{i=0}^{N} x_i^2\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "b * \\overline{x} + k * \\overline{x^2} = \\overline{xy}\n",
    "\\\\\n",
    "\\\\\n",
    "\\overline{y} = b + k *  \\overline{x}\n",
    "\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "выразим b подставим в первое уравнение получим\n",
    "\n",
    "\n",
    "$$\n",
    "k = \\frac{\\overline{xy} - \\overline{x} \\overline{y}}{\\overline{x^2} - \\overline{x}^2}\n",
    "\\\\\n",
    "b = \\overline{y} - k *  \\overline{x}\n",
    "\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:39.004884Z",
     "start_time": "2018-10-24T07:47:38.860635Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head ./weight-height.csv\n",
    "\n",
    "# https://www.kaggle.com/mustafaali96/weight-height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:39.232039Z",
     "start_time": "2018-10-24T07:47:39.149318Z"
    }
   },
   "outputs": [],
   "source": [
    "wh_dataset = np.loadtxt('./weight-height.csv', delimiter=',', skiprows=1, usecols=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:39.810957Z",
     "start_time": "2018-10-24T07:47:39.381293Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:39.848199Z",
     "start_time": "2018-10-24T07:47:39.813212Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./weight-height.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:40.159050Z",
     "start_time": "2018-10-24T07:47:40.136194Z"
    }
   },
   "outputs": [],
   "source": [
    "wh_dataset = df.loc[df.Gender=='Male', ['Height', 'Weight']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:40.988432Z",
     "start_time": "2018-10-24T07:47:40.983810Z"
    }
   },
   "outputs": [],
   "source": [
    "X = wh_dataset[:, 0]\n",
    "y = wh_dataset[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:41.462158Z",
     "start_time": "2018-10-24T07:47:41.144067Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:41.475100Z",
     "start_time": "2018-10-24T07:47:41.472134Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.vstack([np.ones(X.shape), X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:41.911602Z",
     "start_time": "2018-10-24T07:47:41.908087Z"
    }
   },
   "outputs": [],
   "source": [
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:42.400209Z",
     "start_time": "2018-10-24T07:47:42.393577Z"
    }
   },
   "outputs": [],
   "source": [
    "X[:4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = w_0 + w_1 * x_1 + ... + w_m * x_m + \\epsilon\n",
    "$$\n",
    "X - матрица\n",
    "w - вектор-столбец коэффициентов.\n",
    "$$\n",
    "y = X  w + \\epsilon\n",
    "\\\\\n",
    "L = || y - X w ||^2\n",
    "\\\\\n",
    "w^* = argmin\\ L\n",
    "$$\n",
    "$w^*$ - оптимальные параметры\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T13:17:42.275866Z",
     "start_time": "2018-10-24T13:17:42.266654Z"
    }
   },
   "source": [
    "$$\n",
    "Q = (y - Xw)^\\top\\ (y - Xw)\n",
    "\\\\\n",
    "\\frac{d Q}{d w} = -2y^\\top X + 2 w^\\top X^\\top X = 0\n",
    "$$\n",
    "\n",
    "https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf - книжка с готовыми рецептами про матрички\n",
    "\n",
    "\n",
    "$$\n",
    "w^\\top X^\\top X = y^\\top X\n",
    "\\\\\n",
    "X^\\top X w = X^\\top y\n",
    "$$\n",
    "$ \\left(\\mathbf{X}^\\top \\mathbf{X}\\right) - $ инвертируема?\n",
    "\n",
    "$$ \n",
    "\\mathbf{w} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\left(\\mathbf{X}^\\top \\mathbf{X}\\right) - $ инвертируема?\n",
    "\n",
    "1) Она квадратная\n",
    "\n",
    "2) Определитель - не нулевой, если все столбцы линейно независимы.\n",
    "\n",
    "3) какова сложность операции обращения матрицы?\n",
    "\n",
    "\n",
    "А если наша матрица не инвертируема? Значит что у нее есть некоторые линейно зависимые столбцы\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\left(\\mathbf{X}^\\top \\mathbf{X} + \\alpha I \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Время трюков\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X}\\mathbf{w}\n",
    "$$\n",
    "$\\mathbf{X}$ - матрица полного столбцового ранга ?\n",
    "\n",
    "Тогда мы можем взять всевдообратную матрицу \n",
    "$\\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top$\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top \\mathbf{y} \n",
    "$$\n",
    "\n",
    "\n",
    "https://ru.wikipedia.org/wiki/Псевдообратная_матрица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:54.107364Z",
     "start_time": "2018-10-24T07:47:54.076221Z"
    }
   },
   "outputs": [],
   "source": [
    "w = (np.linalg.inv((X.T @ X)) @ X.T) @ y.reshape((y.shape[0], -1)) \n",
    "w = w.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:54.733749Z",
     "start_time": "2018-10-24T07:47:54.729007Z"
    }
   },
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:55.091294Z",
     "start_time": "2018-10-24T07:47:55.083412Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat = (X @ w).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:55.714094Z",
     "start_time": "2018-10-24T07:47:55.481716Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X[:, 1], y, 'r.')\n",
    "plt.plot(X[:, 1], y_hat, 'b--')\n",
    "plt.title( np.sum( (y - y_hat) ** 2 ) );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:56.325860Z",
     "start_time": "2018-10-24T07:47:55.921285Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(y - y_hat, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:47:56.482215Z",
     "start_time": "2018-10-24T07:47:56.476792Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(y - y_hat), np.std(y - y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Наша модель вроде ловит зависимость, но в данных есть некоторый \"шум\", который мы не можем учесть.\n",
    "\n",
    "В нашем случае наверняка есть какие то факторы / признаки / свойства объектов - которых у нас нет. \n",
    "\n",
    "Поэтому можно просто считать их шумом (погрешностью измерений) и попытаться их моделировать нормальным распределением.\n",
    "\n",
    "\n",
    "![](./imgs/regression_bayes.png)\n",
    "\n",
    "credits for image to https://habr.com/post/276355/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{N}(y|\\mu, \\sigma^2) =  \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-\\mu)^2}{2\\sigma^2}\\right),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Нормальное распределение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рост человека, вес человека, отклонение при стрельбе от центра мишени, итд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:49:27.321553Z",
     "start_time": "2018-10-24T07:49:27.055349Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df.loc[df.Gender=='Male', 'Height'].values, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:50:23.022858Z",
     "start_time": "2018-10-24T07:50:22.778560Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df.loc[:, 'Weight'].values, bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T07:50:55.336266Z",
     "start_time": "2018-10-24T07:50:54.941949Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(df.loc[df.Gender=='Male', 'Weight'].values, bins=30, alpha=0.5);\n",
    "plt.hist(df.loc[df.Gender=='Female', 'Weight'].values, bins=30, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Переход в многомерное нелинейное пространство\n",
    "\n",
    "### Как сделать регрессию линейной если зависимость нелинейная?\n",
    "\n",
    "-   $\\mathbf{x}$ может зависеть не совсем линейно от $\\mathbf{y}$.\n",
    "\n",
    "-   Перейдем в новое пространство - $\\phi(\\mathbf{x})$ где $\\phi(\\cdot)$ это нелинейная функция от $\\mathbf{x}$.\n",
    "\n",
    "-   Возьмем линейную комбинацию этих нелинейных функций $$f(\\mathbf{x}) = \\sum_{j=1}^k w_j \\phi_j(\\mathbf{x}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   Возьмем некотрый базис функций (например квадратичный базис)\n",
    "    $$\\boldsymbol{\\phi} = [1, x, x^2].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-2, 2), [1 for x in np.linspace(-2, 2)], label='1')\n",
    "plt.plot(np.linspace(-2, 2), [x for x in np.linspace(-2, 2)], label='x')\n",
    "plt.plot(np.linspace(-2, 2), [x**2 for x in np.linspace(-2, 2)], label='x^2')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-2, 2), \n",
    "         [1.34 * x**2 + 3.8 * x + 9 * 1 \n",
    "          for x in np.linspace(-2, 2)], \n",
    "         label='1.34 * x**2 + 3.8 * x + 9 * 1')\n",
    "\n",
    "plt.plot(np.linspace(-2, 2), \n",
    "         [-2.3* x**2 + 3 * 1 \n",
    "          for x in np.linspace(-2, 2)],\n",
    "         label='-2.3* x**2 + 3 * 1')\n",
    "\n",
    "plt.plot(np.linspace(-2, 2), \n",
    "         [-0.0003 * x**2 + 12 * x -2 * 1 \n",
    "          for x in np.linspace(-2, 2)], \n",
    "         label='-0.0003 * x**2 + 12 * x -2 * 1')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "RBF \n",
    "\n",
    "https://ru.wikipedia.org/wiki/Радиально-базисная_функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-5, 5), \n",
    "         [np.exp(-1.3 * (x + 1.3)**2 )\n",
    "          for x in np.linspace(-5, 5)]);\n",
    "\n",
    "plt.plot(np.linspace(-5, 5), \n",
    "         [np.exp( -1.3 * (x - 2)**2)\n",
    "          for x in np.linspace(-5, 5)]);\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(-2, 2), \n",
    "         [ 1.3 * np.exp(-1.3 * (x + 1.3)**2 ) + 3.5 * np.exp( -1.3 * (x - 2)**2)  - 5 * np.exp( - 5 * (x - 1.3)**2)\n",
    "          for x in np.linspace(-2, 2)], label = 'o!');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-   Теперь наша функция имеет такой вид\n",
    "    $$f(\\mathbf{x}_i) = \\sum_{j=1}^m w_j \\phi_{i, j} (x_i).$$\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:03:36.651702Z",
     "start_time": "2018-10-24T08:03:36.646984Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "X = np.linspace(-5, 5)\n",
    "y = -1.21 * X ** 3 - 2 * X - 5 +  np.random.rand(X.shape[0]) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:03:37.245614Z",
     "start_time": "2018-10-24T08:03:37.057912Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'r.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:03:38.007940Z",
     "start_time": "2018-10-24T08:03:37.993280Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:03:38.428829Z",
     "start_time": "2018-10-24T08:03:38.415515Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=True)\n",
    "pf = PolynomialFeatures(degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:03:38.780025Z",
     "start_time": "2018-10-24T08:03:38.775450Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "X_poly = pf.fit_transform(X.reshape(-1, 1))\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:03:39.181032Z",
     "start_time": "2018-10-24T08:03:39.175937Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "X_poly = np.hstack([X_poly, np.ones((X_poly.shape[0], 1)) ])\n",
    "X_poly[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:03:39.588656Z",
     "start_time": "2018-10-24T08:03:39.582105Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "lr.fit(X_poly, y)\n",
    "y_pred = lr.predict(X_poly)\n",
    "\n",
    "# обучим и предскажем на тех же данных чтобы увидеть что модель достаточно \n",
    "# сложная и способна ловить нелинейные зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:03:58.848141Z",
     "start_time": "2018-10-24T08:03:58.634076Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X, y, 'r.')\n",
    "plt.plot(X, y_pred, 'b-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "> по факту мы только что подобрали такую поверхность с помощью линейной регрессии, отображая куда данные мы можем их линейно аппроксимировать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разложение ошибки на смещение и разброс (Bias-variance decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/bias_var.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как правило, при увеличении сложности модели (например, при увеличении количества свободных параметров) увеличивается дисперсия (разброс) оценки, но уменьшается смещение. Из-за того что тренировочный набор данных полностью запоминается вместо обобщения, небольшие изменения приводят к неожиданным результатам (переобучение). Если же модель слабая, то она не в состоянии выучить закономерность, в результате выучивается что-то другое, смещенное относительно правильного решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регуляризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "b = 10\n",
    "n_points = 40\n",
    "x_min = 0.5\n",
    "x_max = 4\n",
    "\n",
    "x = np.linspace(x_min, x_max, n_points)[:, np.newaxis]\n",
    "completely_random_number = 33\n",
    "rs = np.random.RandomState(completely_random_number)\n",
    "noise = rs.normal(0, 5, (n_points, 1))\n",
    "\n",
    "y = a + b * x + noise\n",
    "idx = np.arange(3,40,4)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x,y, s=80, c ='b', edgecolors='k', linewidths=0.3);\n",
    "plt.scatter(x[idx],y[idx], s=80, c='r');\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x[idx],y[idx], s=80, c ='r', edgecolors='k', linewidths=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = x[idx]\n",
    "y_train = y[idx]\n",
    "\n",
    "lr_linear = LinearRegression(fit_intercept=True)\n",
    "lr_linear.fit(x_train, y_train)\n",
    "y_linear = lr_linear.predict(x_train)\n",
    "\n",
    "# Cubic \n",
    "cubic = PolynomialFeatures(degree=3)\n",
    "x_cubic = cubic.fit_transform(x_train)\n",
    "\n",
    "lr_3 = LinearRegression(fit_intercept=False)\n",
    "lr_3.fit(x_cubic, y_train)\n",
    "y_cubic = lr_3.predict(x_cubic)\n",
    "\n",
    "\n",
    "# 9'th fit\n",
    "poly = PolynomialFeatures(degree=9)\n",
    "x_poly = poly.fit_transform(x_train)\n",
    "\n",
    "lr_9 = LinearRegression(fit_intercept=False)\n",
    "lr_9.fit(x_poly, y_train)\n",
    "y_poly = lr_9.predict(x_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION ON WHOLE DATA\n",
    "\n",
    "# linear prediction\n",
    "\n",
    "y_pred_linear = lr_linear.predict(x)\n",
    "\n",
    "# cubic prediction\n",
    "\n",
    "x_cubic_test = cubic.transform(x)\n",
    "y_pred_cubic = lr_3.predict(x_cubic_test)\n",
    "\n",
    "# poly 9 prediction\n",
    "\n",
    "x_poly_test = poly.transform(x)\n",
    "y_pred_poly = lr_9.predict(x_poly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_plot(x, y, y_regression, test_idx=None):\n",
    "    plt.scatter(x,y, s=80, c ='r', edgecolors='k', linewidths=0.3);\n",
    "    plt.plot(x,y_regression);\n",
    "    if test_idx is not None:\n",
    "        plt.scatter(x[test_idx], y[test_idx], s=80, c ='b', edgecolors='k', linewidths=0.3);\n",
    "    plt.title('MSE = {}'.format(mean_squared_error(y, y_regression)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT PICTURES\n",
    "\n",
    "plt.figure(figsize=(24,12))\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "simple_plot(x_train,y_train,y_linear)\n",
    "plt.subplot(2,3,2)\n",
    "simple_plot(x_train,y_train,y_cubic)\n",
    "plt.subplot(2,3,3)\n",
    "simple_plot(x_train,y_train,y_poly)\n",
    "\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "simple_plot(x,y,y_pred_linear, test_idx=idx)\n",
    "plt.subplot(2,3,5)\n",
    "simple_plot(x,y,y_pred_cubic, test_idx=idx)\n",
    "plt.subplot(2,3,6)\n",
    "simple_plot(x[3:],y[3:],y_pred_poly[3:], test_idx=idx-3)\n",
    "\n",
    "\n",
    "# FIRST ROW is TEST data set, SECOD ROW is WHOLE data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Переобучаемся под обучающую выборку - слишком \"хорошая модель\" которая может ловить мега-нелинейные зависимости, которых на самом деле нет.\n",
    "\n",
    "\n",
    ">> Большие коэффициенты делают возможным сильное изменение величины при небольшом изменении признаков.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:10:11.796333Z",
     "start_time": "2018-10-24T08:10:11.782308Z"
    }
   },
   "source": [
    "##### Подход 0.\n",
    "\n",
    "- Упростить модель.\n",
    "- Добавить больше данных в обучающую выборку\n",
    "\n",
    "##### Подход 1.\n",
    "\n",
    "- Давайте запретим нашим коэффициентам быть большими!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:21:37.256009Z",
     "start_time": "2018-10-24T08:21:37.244366Z"
    }
   },
   "source": [
    "$$\n",
    "Q_{reg}(\\hat{f}, x) = Q(\\hat{f}, x) + \\alpha * R(w) \\to min\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "Можно просто добавить некоторый функционал который будет зависеть от весов регрессии\n",
    "\n",
    "$$\n",
    "MSE(\\hat{f}, x) = \\left|\\left| \\ y - X w\\ \\right|\\right|^2_2 + \\left|\\left| w \\right|\\right|^2_2 \\ \\to min\n",
    "$$\n",
    "\n",
    "![](./imgs/norma.jpg)\n",
    "\n",
    "https://ru.wikipedia.org/wiki/Норма_(математика)\n",
    "\n",
    "\n",
    "\n",
    "Выражение \n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{N}\\left(y_i - (w_2 * x_i ^ 2 + w_1 * x_i + w_0) \\right) ^ 2 + \\alpha \\sum_{j=1}^{2}\\left(w_i\\right)^2 \\to min\n",
    "$$\n",
    "\n",
    "эквивалентно в случае выпуклого функционала MSE\n",
    "\n",
    "Почему можно увидеть тут: (https://ru.wikipedia.org/wiki/Метод_множителей_Лагранжа)\n",
    "и тут\n",
    "(https://ru.wikipedia.org/wiki/Условия_Каруша_—_Куна_—_Таккера)\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\sum_{i=0}^{N}\\left(y_i - (w_2 * x_i ^ 2 + w_1 * x_i + w_0) \\right) ^ 2 \\to min\n",
    "\\\\\n",
    "\\\\\n",
    "\\sum_{j=1}^{2}\\left(w_i\\right)^2 \\leq C\n",
    "\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "То есть мы хотим минимизировать наш функционал MSE, но при этом сделать это так, чтобы у нас веса регрессии были как можно меньше. (Почему мы не занижаем $w_0$ ?\n",
    "(Если у нас нет информации что свободная переменная должна быть близкой к нулю то смысла нет.))\n",
    "\n",
    "Такой вид регрессии называется Ridge Regression - гребневая."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://habrastorage.org/files/c6e/21e/c4d/c6e21ec4d5364b59a727c193760a40e1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T08:46:43.653832Z",
     "start_time": "2018-10-24T08:46:43.637991Z"
    }
   },
   "source": [
    "Почему гребневая?\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\left(\\mathbf{X}^\\top \\mathbf{X} + \\alpha E \\right)^{-1} \\mathbf{X}^\\top \\mathbf{y} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Либо в качестве слагаемого отвечающего за регуляризацию можно добавить сумму модулей значений весов и минимизировать их.\n",
    "\n",
    "$$\n",
    "MSE(\\hat{f}, x) = \\left|\\left| \\ y - X w\\ \\right|\\right|^2_2 + \\left|\\left| w \\right|\\right|_1 \\ \\to min\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{N}\\left(y_i - (w_2 * x_i ^ 2 + w_1 * x_i + w_0) \\right) ^ 2 + \\alpha \\sum_{j=1}^{2}\\left|w_i\\right| \\to min\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\sum_{i=0}^{N}\\left(y_i - (w_2 * x_i ^ 2 + w_1 * x_i + w_0) \\right) ^ 2 \\to min\n",
    "\\\\\n",
    "\\\\\n",
    "\\sum_{j=1}^{2}\\left|w_i\\right | \\leq C\n",
    "\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Такая регрессия называется Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T09:03:49.394889Z",
     "start_time": "2018-10-24T09:03:49.251639Z"
    }
   },
   "source": [
    "![](./imgs/regularization.png)\n",
    "\n",
    "кажется рисунок из книжки \n",
    "https://web.stanford.edu/~hastie/ElemStatLearn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) мы смещаем наши оптимальные оценки на веса регрессии (w или betta  в зависимости от обозначения) - ближе к нулю.\n",
    "т.е. задаем априорные знания о весах в модель - говорим ей что веса должны быть ближе к нулю.\n",
    "\n",
    "2) видно что модуль в большинстве случаем жестко зануляет веса некоторых признаков в отличие от квадрата - таким образом происходит отбор более значимых признаков - т.е. признаки с небольшими весами - не очень интересны.\n",
    "\n",
    "3) что еще можно сказать про регуляризацию ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./imgs/l1_l2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Пример"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:55:21.890011Z",
     "start_time": "2018-10-24T12:55:21.882129Z"
    },
    "nbpresent": {
     "id": "f938960b-ecd9-430d-9923-33a1dc4e6e88"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model, cross_validation\n",
    "from sklearn.cross_validation import cross_val_score, KFold, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:55:22.264872Z",
     "start_time": "2018-10-24T12:55:22.233618Z"
    },
    "nbpresent": {
     "id": "23138de7-cb10-47fe-bd8d-545eb2c7c2a7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:55:22.675566Z",
     "start_time": "2018-10-24T12:55:22.669156Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:55:24.428211Z",
     "start_time": "2018-10-24T12:55:24.419633Z"
    }
   },
   "outputs": [],
   "source": [
    "X = diabetes.data[:, 2]\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:55:24.928750Z",
     "start_time": "2018-10-24T12:55:24.921833Z"
    }
   },
   "outputs": [],
   "source": [
    "X = X[:, np.newaxis]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:55:33.807690Z",
     "start_time": "2018-10-24T12:55:33.793793Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:55:37.308494Z",
     "start_time": "2018-10-24T12:55:37.291840Z"
    },
    "nbpresent": {
     "id": "b9173966-1f89-4533-afa7-917fd3a6f21d"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:55:48.430667Z",
     "start_time": "2018-10-24T12:55:48.422310Z"
    }
   },
   "outputs": [],
   "source": [
    "regr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:57:22.631380Z",
     "start_time": "2018-10-24T12:57:22.614540Z"
    }
   },
   "outputs": [],
   "source": [
    "for alpha in [0.01, 0.05, 0.1, 1.0]:\n",
    "    regr = linear_model.Ridge(alpha=alpha)\n",
    "    regr.fit(X_train, y_train)\n",
    "    print(alpha, regr.coef_)\n",
    "    \n",
    "# alpha - Larger values specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:11.986318Z",
     "start_time": "2018-10-24T12:58:11.956469Z"
    }
   },
   "outputs": [],
   "source": [
    "for alpha in [0.01, 0.05, 0.1, 1.0]:\n",
    "    regr = linear_model.Lasso(alpha=alpha)\n",
    "    regr.fit(X_train, y_train)\n",
    "    print(alpha, regr.coef_)\n",
    "    \n",
    "# alpha - Larger values specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:30.263433Z",
     "start_time": "2018-10-24T12:58:30.255098Z"
    }
   },
   "outputs": [],
   "source": [
    "r2_score(y_test, regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:32.710418Z",
     "start_time": "2018-10-24T12:58:32.509424Z"
    },
    "nbpresent": {
     "id": "da7d7d5c-1120-4482-8d87-b4ef90834a8e"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(X_test, y_test,  color='black')\n",
    "plt.plot(X_test, regr.predict(X_test), color='blue',\n",
    "         linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:35.532375Z",
     "start_time": "2018-10-24T12:58:35.521792Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.logspace(-4, -.5, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:40.452068Z",
     "start_time": "2018-10-24T12:58:40.410820Z"
    }
   },
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:150]\n",
    "y = diabetes.target[:150]\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "alphas = np.logspace(-4, -.5, 30)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "\n",
    "this_scores = cross_validation.cross_val_score(\n",
    "    regr, X, y, n_jobs=1, cv=5,\n",
    "    scoring=make_scorer(r2_score, greater_is_better=True)\n",
    ")\n",
    "\n",
    "print(np.mean(this_scores), '+/-', np.std(this_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:41.145888Z",
     "start_time": "2018-10-24T12:58:40.822393Z"
    },
    "nbpresent": {
     "id": "4771a1f0-3eee-4a60-b271-72ae5c57beec"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:150]\n",
    "y = diabetes.target[:150]\n",
    "\n",
    "lasso = linear_model.Lasso()\n",
    "alphas = np.logspace(-4, -.5, 30)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso.alpha = alpha\n",
    "    this_scores = cross_validation.cross_val_score(\n",
    "        lasso, X, y, n_jobs=1, cv=5,\n",
    "        scoring=make_scorer(r2_score, greater_is_better=True)\n",
    "    )\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:41.718704Z",
     "start_time": "2018-10-24T12:58:41.212449Z"
    },
    "nbpresent": {
     "id": "4a2df9f1-db4d-48ba-a575-d4f301288f79"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.semilogx(alphas, scores)\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "plt.semilogx(alphas, np.array(scores) + np.array(scores_std) / np.sqrt(len(X)),\n",
    "             'b--')\n",
    "plt.semilogx(alphas, np.array(scores) - np.array(scores_std) / np.sqrt(len(X)),\n",
    "             'b--')\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:44.736457Z",
     "start_time": "2018-10-24T12:58:44.730553Z"
    }
   },
   "outputs": [],
   "source": [
    "scores[np.argmax(scores)], '+/-', scores_std[np.argmax(scores)], 'alpha=', alphas[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:45.535057Z",
     "start_time": "2018-10-24T12:58:45.339143Z"
    }
   },
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data[:150]\n",
    "y = diabetes.target[:150]\n",
    "\n",
    "ridge = linear_model.Ridge()\n",
    "alphas = np.logspace(-4, -.5, 30)\n",
    "\n",
    "scores = list()\n",
    "scores_std = list()\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge.alpha = alpha\n",
    "    this_scores = cross_validation.cross_val_score(\n",
    "        ridge, X, y, n_jobs=1, cv=5,\n",
    "        scoring=make_scorer(r2_score, greater_is_better=True)\n",
    "    )\n",
    "    scores.append(np.mean(this_scores))\n",
    "    scores_std.append(np.std(this_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:46.168882Z",
     "start_time": "2018-10-24T12:58:45.737754Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.semilogx(alphas, scores)\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "plt.semilogx(alphas, np.array(scores) + np.array(scores_std) / np.sqrt(len(X)),\n",
    "             'b--')\n",
    "plt.semilogx(alphas, np.array(scores) - np.array(scores_std) / np.sqrt(len(X)),\n",
    "             'b--')\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-24T12:58:46.234145Z",
     "start_time": "2018-10-24T12:58:46.228791Z"
    }
   },
   "outputs": [],
   "source": [
    "print(scores[np.argmax(scores)], '+/-', scores_std[np.argmax(scores)], 'alpha=', alphas[np.argmax(scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix кому интересно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В задаче линейной регрессии мы предполагаем, что данные пораждаются следующим процессом:\n",
    "\n",
    "$$ \\Large y = \\vec{w}^T \\vec{x} + \\epsilon$$\n",
    "\n",
    "- $y$ - целевая переменная\n",
    "- $\\vec{w}$ - вектор параметров модели\n",
    "- $\\vec{x}$ -  вектор признаков объекта\n",
    "- $\\epsilon \\sim \\mathcal{N}(0,\\,\\sigma^{2})\n",
    "$ - случайная ошибка, полученная из нормального распределения с нулевым матожиданием\n",
    "\n",
    "На выходе из эксперимента мы имеем набор данных из $N$ объектов $\\{\\vec{x}_n\\}$ и соответствующие им результаты измерений $\\{y_n\\}$. Наша задача сводится к определению коэффициентов регрессии $\\vec{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Максимизация правдоподобия (likelihood)\n",
    "\n",
    "Для решения этой задачи нам нужно определиться с метрикой качества, которая покажет степень соответствия данной модели и тренировочных данных. Для этого воспользуемся методом максимизации правдоподобия.\n",
    "\n",
    "Правдоподобием называют вероятность того, что данная выборка была семплирована из данного распределения. Если объекты независимы и одинаково распределённы, то правдоподобие вычисляется как:\n",
    "$$\n",
    "    \\Large\n",
    "    \\mathcal {L}_\\theta = \\prod_{i=1}^{N} P_\\theta(\\vec{x}_i)\n",
    "$$\n",
    "\n",
    "Т.к. большинство распределений параметрические, правдоподобие удобно использовать для того, чтобы оценить параметры распределения из которого появилась выборка. Для этого нужно найти такое $\\theta$ при котором правдоподобие будет максимальным. \n",
    "\n",
    "Посмотрим на нашу модель с вероятностной точки зрения.\n",
    "<img src=\"./imgs/2.png\" alt=\"Bishop, Pattern Recognition and Machine Learning, 2006\" style=\"width: 500px;\"/>\n",
    "\n",
    "$$\n",
    "\\Large \n",
    "y = \\mathbb{E}\\left[ p(t|x,w,\\sigma^{2}) \\right]\n",
    "$$\n",
    "\n",
    "Выпишем фунцию правдоподобия для нашего набора данных. Сразу возьмем логарифм правдоподобия, т.к. он поможет нам избавиться от произведения и степени экспоненты в нормальном распределении\n",
    "$$\n",
    "\\Large \n",
    "\\begin{array}{rcl}\n",
    " \\log \\left(\\mathcal {L}\\right) \n",
    "              &=& \\log \\prod_{i=1}^{N} \\mathcal{N}(\\vec{w}^T \\vec{x}_i,\\,\\sigma^{2}) \\\\\n",
    "              &=& \\sum_{i=1}^n \\log \\mathcal{N}\\left( \\vec{w}^T \\vec{x}_i, \\sigma^2 \\right) \\\\ \n",
    "              &=& \\sum_{i=1}^n \\log \\frac {1}{\\sigma {\\sqrt {2\\pi}}}\\;e^{-{\\frac {(y_i-\\vec{w}^T \\vec{x}_i )^{2}}{2\\sigma ^{2}}}} \\\\\n",
    "              &=& -n \\log \\sigma {\\sqrt {2\\pi}} -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \n",
    "                  \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Таким образом задача сводится к максимизации правдоподобия.\n",
    "$$\n",
    "\\Large \n",
    "\\begin{array}{rcl}\n",
    "\\hat{w} &=& \\arg \\max_{w} log\\left(\\mathcal {L}\\right) \\\\ \n",
    "        &=& \\arg \\max_{w} -n \\log \\sigma {\\sqrt {2\\pi}} -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\\\ \n",
    "        &=& \\arg \\max_{w} - \\sum_{i=1}^n \\left(y_i - \\vec{w}^T \\vec{x}_i\\right)^2 \\\\ \n",
    "        &=& \\arg \\min_{w} L\\left(X, \\vec{y}, \\vec{w}\\right)\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ВИДИМ ЧТО В ФУНКЦИИ ОШИБКИ - квадратичный лосс?\n",
    "\n",
    "Если бы брали не нормальное распределение, а распределение лапласса - то увидили бы лосс на MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Аналитическое решение\n",
    "\n",
    "Для того, чтобы найти веса $\\vec{w}$ приравняем к нулю производную функции потерь $L$\n",
    "\n",
    "$$\n",
    "\\Large \n",
    "\\begin{array}{rcl} \n",
    "\\frac{\\partial L}{\\partial \\vec{w}} = 0 \n",
    "    &\\Leftrightarrow& \\frac{1}{2n} \\left(-2 X^T \\vec{y} + 2X^T X \\vec{w}\\right) = 0 \\\\ \n",
    "    &\\Leftrightarrow& -X^T \\vec{y} + X^T X \\vec{w} = 0 \\\\ \n",
    "    &\\Leftrightarrow& X^T X \\vec{w} = X^T \\vec{y} \\\\ \n",
    "    &\\Leftrightarrow& \\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y} \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки аналитического решения:\n",
    "- Вырожденность матрицы $X^TX$\n",
    "- Вычислительная сложность обращения матрицы\n",
    "- Вычислительная стабильность операции обращения"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
